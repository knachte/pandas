{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the columns to the correct datatypes \n",
    "When imported data may get assigned datatypes that are not fitting for them. Depending on what you want them to be there are several ways of converting them:\n",
    "```python\n",
    "df[\"floatValue\"] = df[\"floatAsString\"].astype(float)\n",
    "df['date'] = pd.to_datetime(df['dateAsString'])\n",
    "```\n",
    "To select only columns of a certain type you can use:\n",
    "```python\n",
    "toConvert = df.select_dtypes(include=['int']).columns.tolist()\n",
    "```\n",
    "or\n",
    "```python\n",
    "toConvert = df.dtypes[df.dtypes == np.int64].index.tolist()\n",
    "```\n",
    "With a for loop you can then perform conversions on these:\n",
    "```python\n",
    "# Converting all integer columns to floats\n",
    "toConvert = df.dtypes[df.dtypes == np.int64].index.tolist()\n",
    "for x in toConvert: df[x] = df[x].astype(float)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for NaN's\n",
    "It's probably a good idea to remove a column if more than 30% of it's rows contain Nan's. Check manually with *df.isnull().sum()* which ones to drop to be certain. \n",
    "```python\n",
    "df.isnull().sum()\n",
    "...\n",
    "df.drop(['column'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "Also drop rows where there is no label (what we use as y), as there is no use in training on data that has no value to predict.\n",
    "```python\n",
    "df.dropna(subset=['label_column'],inplace=True)\n",
    "```\n",
    "\n",
    "After that you could replace the still missing values by the mean or median for numerical data and the most occurring category for the others:\n",
    "```python\n",
    "def fill_missing_values(df):\n",
    "    ''' This function imputes missing values with median for numeric columns \n",
    "        and most frequent value for categorical columns'''\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    for column in list(missing.index):\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column].fillna(df[column].value_counts().index[0], inplace=True)\n",
    "        elif df[column].dtype == 'int64' or 'float64' or 'int16' or 'float16':\n",
    "            df[column].fillna(df[column].median(), inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns that represent grades can be converted to numeric\n",
    "This is when a continuous value is represented as a non numeric value, such as a quality representation, a rating, etc. This avoids turning them into multiple columns later. \n",
    "```python\n",
    "df.Quality.replace({\"perfect\":4,\"good\":3,\"acceptable\":2,\"bad\":1,\"unusable\":0}, inplace=True)\n",
    "df[\"Quality\"] = df[\"Quality\"].astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove useless info from useful columns \n",
    "E.g. columns containing temperatures where there is more than what is needed in the data.\n",
    "```python\n",
    "import re\n",
    "my_string = \"temperature: 75.6 F\"\n",
    "pattern = re.compile(\"\\d+\\.\\d+\")\n",
    "out = re.match(pattern, my_string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting parts of a date\n",
    "Sometimes the year part of a date can be good enough for a model to make predictions on.\n",
    "```python\n",
    "df[\"year\"] = df[\"original_datetime\"].apply(lambda row: row.year)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into categorical and numeric values\n",
    "This can be done by effectively splitting up the data into two DataDrames that will be reassembled later on, or making masks that contain only one of the two categories.\n",
    "```python\n",
    "numeric=[]\n",
    "categorical=[]\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        categorical.append(column)\n",
    "    elif df[column].dtype == 'int64' or 'float64' or 'int16' or 'float16':\n",
    "        numeric.append(column)\n",
    "        \n",
    "num = df[numeric]\n",
    "cat = df[categorical]\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**The parts below are applied to the numerical part of the DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data with StandardScaler\n",
    "\n",
    "The StandardScaler transforms the data in such a way that it has it's mean at 0 and std as 1. Standardization is useful for data which has negative values. It arranges the data in normal distribution. It is more useful for classification than regression.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scale selected columns\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "df['column'] = scaler.fit_transform(df[['column']])\n",
    "```\n",
    "Mind the double square brackets in the last example.\n",
    "```python\n",
    "#or to scale an entire DataFrame:\n",
    "df_scaled = scaler.fit_transform(df)    \n",
    "```\n",
    "\n",
    "https://medium.com/predict/standardization-on-crazy-data-python-cd5b1282a97f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation (min max)\n",
    "Normalisation is more useful than the StandardScaler when the data is only positive. It puts everything between 0 and 1.\n",
    "\n",
    "https://medium.com/@equipintelligence/normalization-of-crazy-data-python-4fa6611e7b46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns with high variance are canidates for log normalization\n",
    "Print out the variance of the numerical columns. You can find those with df.var()\n",
    "\n",
    "Apply the log normalization function to the columns with high variance (> 1000):\n",
    "```python\n",
    "df['numericColumn'] = np.log(df.numericColumn)\n",
    "```\n",
    "\n",
    "This will cause problems if there are rows with zero in them. This can be overcome the following way:\n",
    "\n",
    "```python\n",
    "log_col = lambda x: np.log(x) if x>0 else 0\n",
    "\n",
    "for x in (df.var()[df.var()>1000]).index: \n",
    "   dft[x] = dft[x].apply(log_col)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns with low variance can be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns do not contain enough variance in their data to be of use during the processing and can be dropped. This can be done with *VarianceThreshold*. A way to determine the threshold below which to drop columns could be to take the mean of all the variances for the columns and drop those that are a certain amount below, but use common sense to not drop too many or to close to the mean. \n",
    "\n",
    "```python \n",
    "thresholder = VarianceThreshold(threshold=0.5)\n",
    "thresholder.fit(features)\n",
    "features_high_variance = df_num[df_num.columns[thresholder.get_support(indices=True)]]\n",
    "```\n",
    "This keeps the output as a DataFrame that has the originals columns and indexes intact. The normal output from fit_transform strips those away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers\n",
    "Outliers are values that are far away from the rest of the values and that could cause problems with the model. However, outliers that are linked to the Y axis should be kept, e.g. an outlier that gives a higher sales value should be kept.\n",
    "These outliers should be in the training dataset, not in the test.\n",
    "\n",
    "A good starting point could be to remove everything that is more than 3 standard deviations away from the mean. Do keep in mind that if there are values that can be zero, e.g. swimming pool size, you could get points marked as outliers because of the zero's, while they are actually valid points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for skewed data\n",
    "If the data is skewed, apply boxcox or remove the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the correlation between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two columns are extremely colinear (>95%) they should be removed.\n",
    "Techniques to check for this is to use df.corr() and plot an sns.heatmap() of it.\n",
    "Another way to check is with variance_inflation_factor()\n",
    "\n",
    "If two columns have a high correlation, but aren't so high that they could be dropped due to having a correlation above 95% and they are similar values they could be replaced by the mean of the columns.\n",
    "E.g. values from multiple sensors.\n",
    "\n",
    "```python\n",
    "columns=['col1','col2']\n",
    "df[\"mean\"] = df.apply(lambda row: row[columns].mean(), axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "Which features are the most useful to get a good model? A RandomForest can be used to see which parameters it uses for it's predictions. This can then serve as a base to determine what to use and what not. This is not the final RandomForest that will be used to make predictions with.\n",
    "\n",
    "The following function can be used to get all the features that are above a specified importance percentage:\n",
    "```python\n",
    "def random_forest_limits(rf, x_df, limit):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(rf.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 3)) for feature, importance in zip(list(x_df.columns), importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)    \n",
    "    features = []\n",
    "    for x in feature_importances:\n",
    "        if (x[1] >= limit):\n",
    "            features.append(x[0])\n",
    "    return features\n",
    "``` \n",
    "The parameters to pass in are the RandomForest model, the features DataFrame (the X part), and the importance percentage below which not to include the feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**The following applies to the the categorical data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy variables.\n",
    "This step transforms categorical data into data that can be used by the models to make predictions. It is different for columns with only two possible categories (binary) and multiple ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding binary variables\n",
    "These are columns with two possible choices, e.g. Y/N, Male/Female, ...\n",
    "\n",
    "#### Pandas version:\n",
    "```python\n",
    "df[\"encoded_col\"] = df[\"col\"].apply(lambda val: 1 if val == \"y\" else 0)\n",
    "```\n",
    "#### LabelEncoder version:\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[\"encoded_col_le\"] = le.fit_transform(df[\"col\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categories\n",
    "\n",
    "A column that can contain a *limited* number of values, e.g. car brands, colors, etc should be converted to n columns, where n is the number of categories in the original - 1.\n",
    "\n",
    "#### With pd.get_dummies()\n",
    "Using get_dummies in this way will return just the new columns for this categorical column. It's output would have to be concatenated and the original column would need to be deleted from the original DataFrame.\n",
    "```python\n",
    "pd.get_dummies(users[\"fav_color\"])\n",
    "```\n",
    "\n",
    "To do this automatically do the following:\n",
    "```python\n",
    "df = pd.get_dummies(df, columns=[x], drop_first=True)\n",
    "```\n",
    "The drop_first parameter specifies that the dummies trap would occur, giving a useless column in the dataset. The amount of new columns would be equal to the amount of categories and not categories - 1 without it. \n",
    "\n",
    "We could get the problem that some categories are in the training set, but not in the test set or vice versa. The following code can be used to solve that:\n",
    "```python\n",
    "# add missing columns\n",
    "for x in df.columns[~df.columns.isin(df_filtered.columns)]:\n",
    "    df_filtered[x] = 0\n",
    "    \n",
    "# remove extra columns\n",
    "for x in df_filtered.columns[~dft.columns.isin(df.columns)]:\n",
    "    df_filtered.drop(x, axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "#### With OneHotEncoder\n",
    "One hot encoder does not have this problem, as there is a fit and a separate transform that can be applied to the test set.\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X)\n",
    "X = enc.transform(X)\n",
    "XTest = enc.transform(XTest)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another random model can be constructed at this point to determine which of the categorical features are important enough to include into the final model. See the *feature selection* part earlier in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_split\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "```\n",
    "**Stratified sampling**:\n",
    "If we know that the distribution of variables in the y column in the dataset is uneven and we wanted to train a model to try to predict y, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
